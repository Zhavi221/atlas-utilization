{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95539ac",
   "metadata": {},
   "source": [
    "Hello,\n",
    "\n",
    "Below you will find a tutorial on how to use AtlasOpenParser - a tool made for working with ATLAS Open Data\n",
    "\n",
    "We will start by importing and initializing configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1518ef",
   "metadata": {},
   "source": [
    "\n",
    "Some background first, \n",
    "\n",
    "ATLAS is a collaberation that utilizes data from the LHC at cern.\n",
    "ATLAS Open Data provides open access to collision data and simulated events from ATLAS for educational and research purposes.\n",
    "\n",
    "ATLAS Open Data - https://opendata.atlas.cern/ (Learn about the project)\n",
    "\n",
    "CERN Open Data portal - https://opendata.cern.ch/ (Explore the data available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ec894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the tools\n",
    "from src.parse_atlas import parser\n",
    "from src.calculations import physics_calcs, combinatorics\n",
    "import math, awkward as ak, numpy as np\n",
    "import atlasopenmagic as atom\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3a484",
   "metadata": {},
   "source": [
    "atlasopenmagic is a library created by ATLAS.\n",
    "\n",
    "The library acts as an interface for metadata retrieval for ATLAS Open Data.\n",
    "\n",
    "Below is the list of available data releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68825bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "releases = atom.available_releases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "atlasparser_config = {\n",
    "    # ===RELEVANT WHEN USED IN A RESTRICTED MEMORY ENV (A CLUSTER)===\n",
    "    \"max_environment_memory_mb\": None,\n",
    "    # ===RELEASE YEARS TO PARSE THE FILES FROM===  \n",
    "    \"release_years\": [\"2024r-pp\"], \n",
    "    # ===AMOUNT OF THREADS TO USE MULTITHREADING WITH, 4 IS A STABLE AMOUNT===\n",
    "    \"max_threads\": 4, \n",
    "    # ===A THRESHOLD FOR PREVENTING MEMORY OVERLOAD===\n",
    "    \"chunk_yield_threshold_bytes\": 500_000_000, \n",
    "    # ===PATH FOR OUTPUTTING THE LOGS===\n",
    "    \"logging_path\": \"tutorial/logs/\"\n",
    "}\n",
    "\n",
    "pipeline_config = {\n",
    "    # ===AMOUNT OF FILES TO LIMIT EACH YEAR (MAINLY TESTING PURPOSES)===\n",
    "    \"limit_files_per_year\": 10, \n",
    "    # ===SOMETIMES FETCHING METADATA CAN BE LONG, SO SET A TIMEOUT===\n",
    "    \"fetching_metadata_timeout\":60,\n",
    "    # ===PATH FOR OUTPUTTING THE DATA===\n",
    "    \"output_path\": \"tutorial/data/root_files/\", \n",
    "}\n",
    "\n",
    "# ===KINEMATIC CUTS===\n",
    "config_kinematic_cuts = {\n",
    "    \"pt\":{\"min\": 0},\n",
    "    \"eta\":{\"min\": -5,\"max\": 5},\n",
    "    \"phi\":{\"min\": -math.pi, \"max\": math.pi}\n",
    "}\n",
    "\n",
    "# ===FIRST FILTER FOR PARTICLE COUNTS===\n",
    "config_particle_counts = {\n",
    "    \"Electrons\" : {\"min\":1, \"max\": 6},\n",
    "    \"Jets\" : {\"min\":2, \"max\": 8},\n",
    "    \"Muons\" : {\"min\":1, \"max\": 6},\n",
    "    \"Photons\" : {\"min\":0, \"max\": 5}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb41c4e",
   "metadata": {},
   "source": [
    "Start by initilazing the AtlasOpenParser class.\n",
    "\n",
    "All available releases in ATLAS Open Data would also be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the parser with configuration\n",
    "atlasparser = parser.AtlasOpenParser(\n",
    "        chunk_yield_threshold_bytes=atlasparser_config[\"chunk_yield_threshold_bytes\"],\n",
    "        max_threads=atlasparser_config[\"max_threads\"],\n",
    "        logging_path=atlasparser_config[\"logging_path\"],\n",
    "        release_years=atlasparser_config[\"release_years\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18518bd3",
   "metadata": {},
   "source": [
    "Fetch all release years' metadata (By configured release_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_years_file_ids: dict = atlasparser.fetch_record_ids(timeout=pipeline_config[\"fetching_metadata_timeout\"])\n",
    "\n",
    "print(\"====================\")\n",
    "print(\"Fetched years and their record IDs:\")\n",
    "for year, record_ids in release_years_file_ids.items():\n",
    "    print(f\"{year}: {len(record_ids)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054ede5",
   "metadata": {},
   "source": [
    "Limit the files if needed (By configured limit_files_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a336b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.AtlasOpenParser.limit_files_per_year(release_years_file_ids, pipeline_config[\"limit_files_per_year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36877afb",
   "metadata": {},
   "source": [
    "Now we can parse these records using the following pipeline ->\n",
    "\n",
    "1. Call parse_files which yields batches of Awkward Arrays (learn about it here https://awkward-array.org/) according to the configured threshold\n",
    "\n",
    "2. Apply kinematic cuts\n",
    "\n",
    "3. Filter by particle amounts\n",
    "\n",
    "4. And save the output\n",
    "\n",
    "Using AtlasOpenParser class and physics_calcs module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac427c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for events_chunk in atlasparser.parse_files(\n",
    "    release_years_file_ids=release_years_file_ids\n",
    "):\n",
    "    #Show events_chunks structure for EDA\n",
    "    print([(field, events_chunk[field].fields) for field in events_chunk.fields])\n",
    "    \n",
    "    #Cutting by kinematics\n",
    "    cut_events = physics_calcs.filter_events_by_kinematics(\n",
    "        events_chunk, config_kinematic_cuts\n",
    "    ) \n",
    "\n",
    "    #Filtering events by particle amounts\n",
    "    filtered_events = physics_calcs.filter_events_by_particle_counts(\n",
    "        events=cut_events, \n",
    "        particle_counts=config_particle_counts, \n",
    "        is_particle_counts_range=True\n",
    "    ) \n",
    "\n",
    "    #Prepare the events array for ROOT format\n",
    "    root_ready = atlasparser.flatten_for_root(filtered_events)\n",
    "\n",
    "    #Save the file\n",
    "    atlasparser.save_events_as_root(root_ready, pipeline_config[\"output_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ea398",
   "metadata": {},
   "source": [
    "After parsing the files as awkward arrays\n",
    "\n",
    "We can analyze it and use it for research. (Inspect the events_chunk object produced in the loop)\n",
    "\n",
    "FROM HERE THE TUTORIAL IS OUTDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_inv_mass_calc = {\n",
    "   \"input_dir\": \"tutorial/data/root_files\",\n",
    "   \"output_dir\": \"tutorial/data/inv_masses\",\n",
    "   \"objects_to_calculate\": [\n",
    "       \"Electrons\",\n",
    "       \"Muons\",\n",
    "       \"Jets\",\n",
    "       \"Photons\"\n",
    "   ],\n",
    "   #===SLICE TOP X EVENTS BY COMBINATION BY THIS FIELD===\n",
    "   \"field_to_slice_by\":\"pt\",\n",
    "   #===MINIMUM AND MAXIMUM PARTICLES FOR IN EACH COMBINATION BY COMBINATORICS===\n",
    "    \"min_particles\": 2,\n",
    "    \"max_particles\": 4,\n",
    "    #===MINIMUM AND MAXIMUM OF HOW MUCH OF EACH PARTICLE IN A COMBINATION===\n",
    "    \"min_count\": 2,\n",
    "    \"max_count\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa36d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_masses(combinations):\n",
    "    for filename in os.listdir(config_inv_mass_calc[\"input_dir\"]):\n",
    "        if filename.endswith(\".root\"):\n",
    "            print(filename)\n",
    "            file_path = os.path.join(config_inv_mass_calc[\"input_dir\"], filename)\n",
    "            \n",
    "            particle_arrays: ak.Array = parser.AtlasOpenParser.parse_root_file(file_path)\n",
    "            print(particle_arrays)\n",
    "            fs_im_mapping = {}\n",
    "            for cur_fs, fs_events in physics_calcs.group_by_final_state(particle_arrays):\n",
    "                if cur_fs not in fs_im_mapping:\n",
    "                    fs_im_mapping[cur_fs] = {}\n",
    "\n",
    "                for combination in combinations:\n",
    "                    if not physics_calcs.is_finalstate_contain_combination(cur_fs, combination):\n",
    "                        continue\n",
    "\n",
    "                    filtered_events: ak.Array = physics_calcs.filter_events_by_particle_counts(\n",
    "                        events=fs_events, \n",
    "                        particle_counts=combination\n",
    "                    )    \n",
    "\n",
    "                    sliced_events_by_pt: ak.Array = physics_calcs.slice_events_by_field(\n",
    "                        events=filtered_events, \n",
    "                        particle_counts=combination,\n",
    "                        field_to_slice_by=config_inv_mass_calc[\"field_to_slice_by\"]\n",
    "                    )\n",
    "\n",
    "                    if len(sliced_events_by_pt) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    cur_im: list = physics_calcs.calc_inv_mass(sliced_events_by_pt) \n",
    "                    \n",
    "                    if not ak.any(cur_im):\n",
    "                        continue\n",
    "                    \n",
    "                    cur_combination_name = prepare_im_combination_name(filename, cur_fs, combination)\n",
    "                    \n",
    "                    if cur_combination_name not in fs_im_mapping[cur_fs]:\n",
    "                        fs_im_mapping[cur_fs][cur_combination_name] = cur_im\n",
    "                    else:\n",
    "                        combination_im = fs_im_mapping[cur_fs][cur_combination_name]\n",
    "                        fs_im_mapping[cur_fs][cur_combination_name] = combination_im.extend(cur_im)\n",
    "              \n",
    "\n",
    "def save_mass_arrays(fs_im_mapping):  \n",
    "    for fs, im_combinations in fs_im_mapping.items():\n",
    "        for im_combination_name, im_arr in im_combinations.items():\n",
    "            output_path = os.path.join(\n",
    "                config_inv_mass_calc[\"output_dir\"], \n",
    "                f\"{im_combination_name}.npy\" \n",
    "                )\n",
    "            \n",
    "            np.save(output_path, ak.to_numpy(im_arr))\n",
    "\n",
    "def prepare_im_combination_name(filename, final_state, combination: dict) -> str:\n",
    "    combination_name = ''\n",
    "    combination_name += f\"{filename}_FS_{final_state}_IM_\"\n",
    "    combination_name = get_combination_dict_repr(combination, combination_name)\n",
    "\n",
    "    return combination_name\n",
    "\n",
    "def get_combination_dict_repr(combination, combination_name=\"\"):\n",
    "    for object, amount in combination.items():\n",
    "        combination_name += f\"{amount}{object[0].lower()}_\"\n",
    "    \n",
    "    return combination_name\n",
    "\n",
    "os.makedirs(config_inv_mass_calc[\"output_dir\"], exist_ok=True)\n",
    "    \n",
    "all_combinations = combinatorics.get_all_combinations(\n",
    "    config_inv_mass_calc[\"objects_to_calculate\"],\n",
    "    min_particles=config_inv_mass_calc[\"min_particles\"],\n",
    "    max_particles=config_inv_mass_calc[\"max_particles\"],\n",
    "    min_count=config_inv_mass_calc[\"min_count\"],\n",
    "    max_count=config_inv_mass_calc[\"max_count\"],\n",
    "    limit=10)\n",
    "\n",
    "inv_mass_arrays = calculate_masses(all_combinations)\n",
    "print(\"===HOW THE MASSES ARE SAVED: ===\")\n",
    "print(inv_mass_arrays)\n",
    "save_mass_arrays(inv_mass_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c565bc",
   "metadata": {},
   "source": [
    "Now we have invariant masses for all of the combinations calculated (here they are limited to 30 but it is configruable)\n",
    "\n",
    "Let's create a histogram with the biggest invariant mass we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all .npy files\n",
    "inv_masses_raw = [f for f in os.listdir(config_inv_mass_calc[\"output_dir\"]) if f.endswith(\".npy\")]\n",
    "inv_masses_sizes = {f: os.path.getsize(os.path.join(config_inv_mass_calc[\"output_dir\"], f)) for f in inv_masses_raw}\n",
    "biggest_file = max(inv_masses_sizes, key=inv_masses_sizes.get)\n",
    "print(f\"Chosen largest file: {biggest_file} ({inv_masses_sizes[biggest_file]} bytes)\")\n",
    "\n",
    "# Sort files by size and take the 9 largest\n",
    "sorted_files = sorted(inv_masses_sizes.items(), key=lambda x: x[1], reverse=True)[:9]\n",
    "selected_files = [f[0] for f in sorted_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67218ea",
   "metadata": {},
   "source": [
    "Now let's plot the top 9 biggest invariant mass arrays side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73af88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram parameters\n",
    "n_bins = 100\n",
    "mass_min, mass_max = 0, 5000000\n",
    "\n",
    "# Create 3x3 subplot grid\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Process each file\n",
    "for idx, filename in enumerate(selected_files):\n",
    "    file_path = os.path.join(config_inv_mass_calc[\"output_dir\"], filename)\n",
    "    inv_masses = np.load(file_path)\n",
    "\n",
    "    # Create histogram\n",
    "    bin_content, bin_edges = np.histogram(inv_masses, bins=n_bins, range=(mass_min, mass_max))\n",
    "    bin_errors = np.sqrt(bin_content)\n",
    "\n",
    "    # Plot on corresponding subplot\n",
    "    ax = axes[idx]\n",
    "    ax.hist(bin_edges[:-1], bins=bin_edges, weights=bin_content, alpha=0.7, color='steelblue')\n",
    "\n",
    "    # Extract particle combination from filename\n",
    "    # Format: year_<file_name>.root_FS_<final_sate>_IM_<combination>_.npy\n",
    "    particle_combo = filename.split('FS')[0].split('.root_')[1]\n",
    "\n",
    "    ax.set_xlabel('Invariant Mass')\n",
    "    ax.set_ylabel('Events')\n",
    "    ax.set_title(f'{filename}\\n({len(inv_masses)} events)', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.suptitle('Invariant Mass Distributions', fontsize=16, y=1.002)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
