tasks:
  do_parsing: false
  do_mass_calculating: true

parsing:
  parse_in_subprocess: true #important for cluster processing
  random_files: false #for testing
  release_years: [] #when empty will parse all available years
  retry_failed_files: true #FEATURE implement this
  max_threads: 4
  max_environment_memory_mb: 8192
  chunk_yield_threshold_bytes: 1000000000 #1GB 
  # chunk_yield_threshold_bytes: 50000000 #FOR TESTING - 50MB for testing
  limit_files_per_year: 50 #FOR TESTING - limit number of files per year, set to -1 to parse all files
  output_path: "data/root_files"
  logging_path: "logs/"
  # crashed_files: "crashed_files.json"
  is_reinitialize_statistics: true
  possible_tree_names: ["CollectionTree", "mini", "analysis"] #TODO explain this paramter here or in docs,
  fetching_metadata_timeout: 30

  particle_counts:
    Electrons:
      min: 1
      max: 6
    Muons:
      min: 1
      max: 6
    Jets:
      min: 2
      max: 8
    Photons:
      min: 0
      max: 5

  kinematic_cuts:
    pt:
      min: 0
    eta:
      min: -5
      max: 5
    phi:
      min: -3.141592653589793
      max: 3.141592653589793

mass_calculate:
  # fs_mapping_threshold_bytes: 1000000000 #1GB 
  fs_mapping_threshold_bytes: 3000 #FOR TESTING - 500MB for testing
  input_dir: "data/root_files"
  output_dir: "data/inv_masses"
  objects_to_calculate:
      - Electrons
      - Muons
      - Jets
      - Photons
  min_particles: 2
  max_particles: 4
  min_count: 2
  max_count: 4
