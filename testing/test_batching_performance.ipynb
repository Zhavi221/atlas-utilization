{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batching Performance Test for `parse_root_file`\n",
        "\n",
        "This notebook tests the batching performance in `parse_root_file`, specifically focusing on:\n",
        "- Different batch sizes\n",
        "- Batched vs non-batched performance\n",
        "- Time breakdown for each step (reading, concatenating, zipping)\n",
        "- Memory usage patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import awkward as ak\n",
        "import uproot\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from contextlib import contextmanager\n",
        "import itertools\n",
        "\n",
        "from src.parse_atlas import parser\n",
        "from src.parse_atlas import schemas\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find Test Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 22 ROOT files\n",
            "\n",
            "Top files to test:\n",
            "                         filename     size_mb\n",
            "2024r-pp_mc_b1c18ef224d66cb7.root 2292.645560\n",
            "   2024r-pp_f2fbf9c5651948e9.root 2147.924682\n",
            "   2024r-pp_8bc3f9c021427cc5.root 1941.396151\n",
            "   2024r-pp_9cbe371e887f8663.root 1865.965141\n",
            "2024r-pp_mc_1e620f3ce6894e8e.root 1580.350282\n",
            "\n",
            "Selected 3 test files\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "input_dir = \"/storage/agrp/netalev/data/root_files/\"  # Update this path if needed\n",
        "\n",
        "# Get ROOT files sorted by size\n",
        "if os.path.exists(input_dir):\n",
        "    root_files = [f for f in os.listdir(input_dir) if f.endswith(\".root\")]\n",
        "    root_files = sorted(root_files, key=lambda x: os.path.getsize(os.path.join(input_dir, x)), reverse=True)\n",
        "    \n",
        "    print(f\"Found {len(root_files)} ROOT files\")\n",
        "    \n",
        "    # Show file sizes\n",
        "    file_sizes = []\n",
        "    for f in root_files[:5]:  # Top 5 largest\n",
        "        size_mb = os.path.getsize(os.path.join(input_dir, f)) / (1024**2)\n",
        "        file_sizes.append({'filename': f, 'size_mb': size_mb})\n",
        "    \n",
        "    df_files = pd.DataFrame(file_sizes)\n",
        "    print(\"\\nTop files to test:\")\n",
        "    print(df_files.to_string(index=False))\n",
        "    \n",
        "    # Select test files: one large, one medium, one small\n",
        "    test_files = []\n",
        "    if len(root_files) >= 3:\n",
        "        test_files = [\n",
        "            (root_files[0], \"large\"),\n",
        "            (root_files[len(root_files)//2], \"medium\"),\n",
        "            (root_files[-1], \"small\")\n",
        "        ]\n",
        "    elif len(root_files) >= 1:\n",
        "        test_files = [(root_files[0], \"available\")]\n",
        "    \n",
        "    print(f\"\\nSelected {len(test_files)} test files\")\n",
        "else:\n",
        "    print(f\"Directory {input_dir} not found. Please update the path.\")\n",
        "    test_files = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instrumented Version of parse_root_file with Timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_root_file_instrumented(file_index, tree_names=[\"CollectionTree\"], batch_size=40_000, return_timings=False):\n",
        "    \"\"\"\n",
        "    Instrumented version of parse_root_file that tracks timing for each step.\n",
        "    \"\"\"\n",
        "    timings = {\n",
        "        'open_file': 0,\n",
        "        'get_tree': 0,\n",
        "        'extract_branches': 0,\n",
        "        'read_batches': 0,\n",
        "        'concatenate': 0,\n",
        "        'zip_objects': 0,\n",
        "        'total': 0\n",
        "    }\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # Open file\n",
        "    open_start = time.time()\n",
        "    with uproot.open(file_index) as root_file:\n",
        "        timings['open_file'] = time.time() - open_start\n",
        "        \n",
        "        # Get tree\n",
        "        tree_start = time.time()\n",
        "        root_file_keys = root_file.keys()\n",
        "        tree_name = parser.AtlasOpenParser.get_data_tree_name_for_root_file(root_file_keys, tree_names)\n",
        "        tree = root_file[tree_name]\n",
        "        all_tree_branches = set(tree.keys())\n",
        "        n_entries = tree.num_entries\n",
        "        is_file_big = n_entries > batch_size if batch_size else False\n",
        "        timings['get_tree'] = time.time() - tree_start\n",
        "        \n",
        "        # Extract branches\n",
        "        extract_start = time.time()\n",
        "        obj_branches_and_quantities = parser.AtlasOpenParser.extract_branches_by_obj_in_schema(\n",
        "            all_tree_branches)\n",
        "        \n",
        "        if not obj_branches_and_quantities.values():\n",
        "            timings['total'] = time.time() - total_start\n",
        "            return (None, timings) if return_timings else None\n",
        "        \n",
        "        all_branches = set(itertools.chain.from_iterable(obj_branches_and_quantities.values()))\n",
        "        timings['extract_branches'] = time.time() - extract_start\n",
        "        \n",
        "        # Initialize storage\n",
        "        obj_events_by_quantities = {obj_name: [] for obj_name in obj_branches_and_quantities.keys()}\n",
        "        \n",
        "        # Define entry ranges\n",
        "        entry_ranges = []\n",
        "        if is_file_big:\n",
        "            entry_ranges = [\n",
        "                (start, min(start + batch_size, n_entries)) \n",
        "                for start in range(0, n_entries, batch_size)\n",
        "            ]\n",
        "        else:\n",
        "            entry_ranges = [(0, n_entries)]\n",
        "        \n",
        "        # Read batches\n",
        "        read_start = time.time()\n",
        "        for entry_start, entry_stop in entry_ranges:\n",
        "            batch_data = tree.arrays(all_branches, entry_start=entry_start, entry_stop=entry_stop)\n",
        "            \n",
        "            for obj_name, branch_mapping in obj_branches_and_quantities.items():\n",
        "                subset = batch_data[list(branch_mapping.keys())]\n",
        "                if len(subset) > 0:\n",
        "                    obj_events_by_quantities[obj_name].append(subset)\n",
        "        timings['read_batches'] = time.time() - read_start\n",
        "        \n",
        "        # Concatenate\n",
        "        concat_start = time.time()\n",
        "        for obj_name, chunks in obj_events_by_quantities.items():\n",
        "            concatenated = ak.concatenate(chunks)\n",
        "            obj_events_by_quantities[obj_name] = concatenated\n",
        "        timings['concatenate'] = time.time() - concat_start\n",
        "        \n",
        "        # Zip objects\n",
        "        zip_start = time.time()\n",
        "        for obj_name, concatenated in obj_events_by_quantities.items():\n",
        "            obj_events_by_quantities[obj_name] = ak.zip({\n",
        "                quantity: concatenated[full_branch]\n",
        "                for full_branch, quantity in obj_branches_and_quantities[obj_name].items()\n",
        "            })\n",
        "        \n",
        "        result = ak.zip(obj_events_by_quantities, depth_limit=1)\n",
        "        timings['zip_objects'] = time.time() - zip_start\n",
        "    \n",
        "    timings['total'] = time.time() - total_start\n",
        "    \n",
        "    return (result, timings) if return_timings else result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "\n",
            "================================================================================\n",
            "Testing file: 2024r-pp_mc_b1c18ef224d66cb7.root\n",
            "File size: 0.01 MB\n",
            "Number of entries: 20,202,169\n",
            "================================================================================\n",
            "\n",
            "Batch size:   no_batch\n",
            "  Total time: 25.73s\n",
            "  â”œâ”€ Open file: 0.003s (0.0%)\n",
            "  â”œâ”€ Get tree: 0.027s (0.1%)\n",
            "  â”œâ”€ Extract branches: 0.000s (0.0%)\n",
            "  â”œâ”€ Read batches: 25.372s (98.6%) - 1 batches\n",
            "  â”œâ”€ Concatenate: 0.001s (0.0%)\n",
            "  â””â”€ Zip objects: 0.331s (1.3%)\n",
            "  Memory used: 3895.5 MB\n",
            "  Throughput: 0.0 MB/s\n",
            "âŒ Batch size 10000: Error - 'FileNotFoundError' object is not subscriptable\n",
            "âŒ Batch size 20000: Error - [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d6\n",
            "âŒ Batch size 40000: Error - [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3952124/1889418538.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3952124/2388208990.py\", line 60, in parse_root_file_instrumented\n",
            "    batch_data = tree.arrays(all_branches, entry_start=entry_start, entry_stop=entry_stop)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/behaviors/TBranch.py\", line 888, in arrays\n",
            "    _ranges_or_baskets_to_arrays(\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/behaviors/TBranch.py\", line 3198, in _ranges_or_baskets_to_arrays\n",
            "    uproot.source.futures.delayed_raise(*obj)\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/futures.py\", line 38, in delayed_raise\n",
            "    raise exception_value.with_traceback(traceback)\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/behaviors/TBranch.py\", line 3106, in chunk_to_basket\n",
            "    basket = uproot.models.TBasket.Model_TBasket.read(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/model.py\", line 854, in read\n",
            "    self.read_members(chunk, cursor, context, file)\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/models/TBasket.py\", line 227, in read_members\n",
            "    ) = cursor.fields(chunk, _tbasket_format1, context)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/cursor.py\", line 201, in fields\n",
            "    return format.unpack(chunk.get(start, stop, self, context))\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/chunk.py\", line 446, in get\n",
            "    self.wait(insist=stop)\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/chunk.py\", line 388, in wait\n",
            "    self._raw_data = numpy.frombuffer(self._future.result(), dtype=self._dtype)\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/coalesce.py\", line 39, in result\n",
            "    return self._parent.result(timeout=timeout)[self._s]\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
            "TypeError: 'FileNotFoundError' object is not subscriptable\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3952124/1889418538.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3952124/2388208990.py\", line 19, in parse_root_file_instrumented\n",
            "    with uproot.open(file_index) as root_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 142, in open\n",
            "    file = ReadOnlyFile(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 573, in __init__\n",
            "    self._begin_chunk = self._source.chunk(\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/fsspec.py\", line 95, in chunk\n",
            "    data = self._fs.cat_file(self._file_path, start=start, end=stop)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 802, in cat_file\n",
            "    with self.open(path, \"rb\", **kwargs) as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 1349, in open\n",
            "    f = self._open(\n",
            "        ^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 210, in _open\n",
            "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 387, in __init__\n",
            "    self._open()\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 392, in _open\n",
            "    self.f = open(self.path, mode=self.mode)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d66cb7.root'\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3952124/1889418538.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3952124/2388208990.py\", line 19, in parse_root_file_instrumented\n",
            "    with uproot.open(file_index) as root_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 142, in open\n",
            "    file = ReadOnlyFile(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 573, in __init__\n",
            "    self._begin_chunk = self._source.chunk(\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/fsspec.py\", line 95, in chunk\n",
            "    data = self._fs.cat_file(self._file_path, start=start, end=stop)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 802, in cat_file\n",
            "    with self.open(path, \"rb\", **kwargs) as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 1349, in open\n",
            "    f = self._open(\n",
            "        ^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 210, in _open\n",
            "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 387, in __init__\n",
            "    self._open()\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 392, in _open\n",
            "    self.f = open(self.path, mode=self.mode)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d66cb7.root'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Batch size 80000: Error - [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d6\n",
            "âŒ Batch size 160000: Error - [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d6\n",
            "File not found: /storage/agrp/netalev/data/root_files/2024r-pp_0fced08cf06527b7.root\n",
            "File not found: /storage/agrp/netalev/data/root_files/2024r-pp_4c4043706f0d183b.root\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3952124/1889418538.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3952124/2388208990.py\", line 19, in parse_root_file_instrumented\n",
            "    with uproot.open(file_index) as root_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 142, in open\n",
            "    file = ReadOnlyFile(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 573, in __init__\n",
            "    self._begin_chunk = self._source.chunk(\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/fsspec.py\", line 95, in chunk\n",
            "    data = self._fs.cat_file(self._file_path, start=start, end=stop)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 802, in cat_file\n",
            "    with self.open(path, \"rb\", **kwargs) as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 1349, in open\n",
            "    f = self._open(\n",
            "        ^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 210, in _open\n",
            "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 387, in __init__\n",
            "    self._open()\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 392, in _open\n",
            "    self.f = open(self.path, mode=self.mode)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d66cb7.root'\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3952124/1889418538.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3952124/2388208990.py\", line 19, in parse_root_file_instrumented\n",
            "    with uproot.open(file_index) as root_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 142, in open\n",
            "    file = ReadOnlyFile(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/reading.py\", line 573, in __init__\n",
            "    self._begin_chunk = self._source.chunk(\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/uproot/source/fsspec.py\", line 95, in chunk\n",
            "    data = self._fs.cat_file(self._file_path, start=start, end=stop)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 802, in cat_file\n",
            "    with self.open(path, \"rb\", **kwargs) as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/spec.py\", line 1349, in open\n",
            "    f = self._open(\n",
            "        ^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 210, in _open\n",
            "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 387, in __init__\n",
            "    self._open()\n",
            "  File \"/srv01/agrp/netalev/.local/lib/python3.11/site-packages/fsspec/implementations/local.py\", line 392, in _open\n",
            "    self.f = open(self.path, mode=self.mode)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/storage/agrp/netalev/data/root_files/2024r-pp_mc_b1c18ef224d66cb7.root'\n"
          ]
        }
      ],
      "source": [
        "def test_batch_sizes(file_path, filename, batch_sizes=[None, 10_000, 20_000, 40_000, 80_000, 160_000]):\n",
        "    \"\"\"\n",
        "    Test parsing with different batch sizes.\n",
        "    None means no batching (read entire file at once).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # First, get file info\n",
        "    with uproot.open(file_path) as root_file:\n",
        "        root_file_keys = root_file.keys()\n",
        "        tree_name = parser.AtlasOpenParser.get_data_tree_name_for_root_file(root_file_keys, [\"CollectionTree\"])\n",
        "        tree = root_file[tree_name]\n",
        "        n_entries = tree.num_entries\n",
        "        print(tree[tree.keys()[0]] == 0)\n",
        "        file_size_mb = tree.num_bytes  / (1024**2)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing file: {filename}\")\n",
        "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
        "    print(f\"Number of entries: {n_entries:,}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    for batch_size in batch_sizes:\n",
        "        # Clean up before each test\n",
        "        gc.collect()\n",
        "        \n",
        "        # Get memory before\n",
        "        process = psutil.Process()\n",
        "        mem_before = process.memory_info().rss / (1024**2)\n",
        "        \n",
        "        # If batch_size is None, use a very large value to effectively disable batching\n",
        "        effective_batch_size = batch_size if batch_size is not None else n_entries + 1\n",
        "        \n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            result, timings = parse_root_file_instrumented(\n",
        "                file_path, \n",
        "                batch_size=effective_batch_size,\n",
        "                return_timings=True\n",
        "            )\n",
        "            \n",
        "            if result is None:\n",
        "                print(f\"âš ï¸  Batch size {batch_size}: Failed to parse\")\n",
        "                continue\n",
        "            \n",
        "            parse_time = time.time() - start_time\n",
        "            \n",
        "            # Get memory after\n",
        "            mem_after = process.memory_info().rss / (1024**2)\n",
        "            mem_used = mem_after - mem_before\n",
        "            \n",
        "            num_events = len(result)\n",
        "            num_batches = max(1, int(np.ceil(n_entries / effective_batch_size))) if batch_size else 1\n",
        "            \n",
        "            result_dict = {\n",
        "                'filename': filename,\n",
        "                'file_size_mb': file_size_mb,\n",
        "                'n_entries': n_entries,\n",
        "                'batch_size': batch_size if batch_size else 'no_batch',\n",
        "                'num_batches': num_batches,\n",
        "                'total_time': timings['total'],\n",
        "                'open_file_time': timings['open_file'],\n",
        "                'get_tree_time': timings['get_tree'],\n",
        "                'extract_branches_time': timings['extract_branches'],\n",
        "                'read_batches_time': timings['read_batches'],\n",
        "                'concatenate_time': timings['concatenate'],\n",
        "                'zip_objects_time': timings['zip_objects'],\n",
        "                'memory_used_mb': mem_used,\n",
        "                'num_events': num_events,\n",
        "                'throughput_mb_per_sec': file_size_mb / timings['total'] if timings['total'] > 0 else 0,\n",
        "                'time_per_event_ms': (timings['total'] / num_events * 1000) if num_events > 0 else 0\n",
        "            }\n",
        "            \n",
        "            results.append(result_dict)\n",
        "            \n",
        "            print(f\"\\nBatch size: {batch_size if batch_size else 'no_batch':>10}\")\n",
        "            print(f\"  Total time: {timings['total']:.2f}s\")\n",
        "            print(f\"  â”œâ”€ Open file: {timings['open_file']:.3f}s ({timings['open_file']/timings['total']*100:.1f}%)\")\n",
        "            print(f\"  â”œâ”€ Get tree: {timings['get_tree']:.3f}s ({timings['get_tree']/timings['total']*100:.1f}%)\")\n",
        "            print(f\"  â”œâ”€ Extract branches: {timings['extract_branches']:.3f}s ({timings['extract_branches']/timings['total']*100:.1f}%)\")\n",
        "            print(f\"  â”œâ”€ Read batches: {timings['read_batches']:.3f}s ({timings['read_batches']/timings['total']*100:.1f}%) - {num_batches} batches\")\n",
        "            print(f\"  â”œâ”€ Concatenate: {timings['concatenate']:.3f}s ({timings['concatenate']/timings['total']*100:.1f}%)\")\n",
        "            print(f\"  â””â”€ Zip objects: {timings['zip_objects']:.3f}s ({timings['zip_objects']/timings['total']*100:.1f}%)\")\n",
        "            print(f\"  Memory used: {mem_used:.1f} MB\")\n",
        "            print(f\"  Throughput: {result_dict['throughput_mb_per_sec']:.1f} MB/s\")\n",
        "            \n",
        "            # Clean up\n",
        "            del result\n",
        "            gc.collect()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Batch size {batch_size}: Error - {str(e)[:100]}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run tests on available files\n",
        "all_results = []\n",
        "import json\n",
        "# test_files = json.load(open(\"/storage/agrp/netalev/data/batch_job_file_ids.json\"))[\"2024r-pp\"]\n",
        "input_dir = \"/storage/agrp/netalev/data/root_files/\"  # Update this path if needed\n",
        "\n",
        "for filename, size in test_files:\n",
        "    file_path = os.path.join(input_dir, filename)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        results = test_batch_sizes(file_path, filename)\n",
        "        all_results.extend(results)\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No results to analyze. Please run the test cells above.\n"
          ]
        }
      ],
      "source": [
        "if all_results:\n",
        "    df_results = pd.DataFrame(all_results)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Group by file and show comparison\n",
        "    for filename in df_results['filename'].unique():\n",
        "        file_df = df_results[df_results['filename'] == filename].copy()\n",
        "        file_df = file_df.sort_values('total_time')\n",
        "        \n",
        "        print(f\"\\nðŸ“ {filename} ({file_df.iloc[0]['file_size_mb']:.1f} MB, {file_df.iloc[0]['n_entries']:,} entries)\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        # Show best and worst\n",
        "        best = file_df.iloc[0]\n",
        "        worst = file_df.iloc[-1]\n",
        "        \n",
        "        print(f\"ðŸ† Best: batch_size={best['batch_size']}, time={best['total_time']:.2f}s\")\n",
        "        print(f\"   Read batches: {best['read_batches_time']:.2f}s ({best['read_batches_time']/best['total_time']*100:.1f}%)\")\n",
        "        print(f\"   Concatenate: {best['concatenate_time']:.2f}s ({best['concatenate_time']/best['total_time']*100:.1f}%)\")\n",
        "        print(f\"   Zip: {best['zip_objects_time']:.2f}s ({best['zip_objects_time']/best['total_time']*100:.1f}%)\")\n",
        "        \n",
        "        print(f\"\\nâŒ Worst: batch_size={worst['batch_size']}, time={worst['total_time']:.2f}s\")\n",
        "        print(f\"   Read batches: {worst['read_batches_time']:.2f}s ({worst['read_batches_time']/worst['total_time']*100:.1f}%)\")\n",
        "        print(f\"   Concatenate: {worst['concatenate_time']:.2f}s ({worst['concatenate_time']/worst['total_time']*100:.1f}%)\")\n",
        "        print(f\"   Zip: {worst['zip_objects_time']:.2f}s ({worst['zip_objects_time']/worst['total_time']*100:.1f}%)\")\n",
        "        \n",
        "        speedup = worst['total_time'] / best['total_time']\n",
        "        print(f\"\\nâš¡ Speedup: {speedup:.2f}x faster with best batch size\")\n",
        "    \n",
        "    # Show detailed breakdown\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DETAILED BREAKDOWN\")\n",
        "    print(\"=\"*80)\n",
        "    print(df_results.to_string(index=False))\n",
        "else:\n",
        "    print(\"No results to analyze. Please run the test cells above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if all_results:\n",
        "    df_results = pd.DataFrame(all_results)\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # 1. Total time vs batch size\n",
        "    ax1 = axes[0, 0]\n",
        "    for filename in df_results['filename'].unique():\n",
        "        file_df = df_results[df_results['filename'] == filename].copy()\n",
        "        # Sort by batch_size, handling 'no_batch' string\n",
        "        file_df['batch_size_num'] = file_df['batch_size'].apply(lambda x: float('inf') if x == 'no_batch' else x)\n",
        "        file_df = file_df.sort_values('batch_size_num')\n",
        "        \n",
        "        batch_sizes_plot = [str(bs) if bs != 'no_batch' else 'no_batch' for bs in file_df['batch_size']]\n",
        "        ax1.plot(batch_sizes_plot, file_df['total_time'], marker='o', label=filename[:30])\n",
        "    \n",
        "    ax1.set_xlabel('Batch Size')\n",
        "    ax1.set_ylabel('Total Time (s)')\n",
        "    ax1.set_title('Total Parse Time vs Batch Size')\n",
        "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. Time breakdown (stacked bar)\n",
        "    ax2 = axes[0, 1]\n",
        "    if len(df_results) > 0:\n",
        "        # Take first file for detailed breakdown\n",
        "        first_file = df_results['filename'].iloc[0]\n",
        "        file_df = df_results[df_results['filename'] == first_file].copy()\n",
        "        file_df = file_df.sort_values('total_time')\n",
        "        \n",
        "        batch_labels = [str(bs) if bs != 'no_batch' else 'no_batch' for bs in file_df['batch_size']]\n",
        "        \n",
        "        bottom = np.zeros(len(file_df))\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "        \n",
        "        for i, (col, color) in enumerate([\n",
        "            ('open_file_time', colors[0]),\n",
        "            ('get_tree_time', colors[1]),\n",
        "            ('extract_branches_time', colors[2]),\n",
        "            ('read_batches_time', colors[3]),\n",
        "            ('concatenate_time', colors[4]),\n",
        "            ('zip_objects_time', colors[5])\n",
        "        ]):\n",
        "            ax2.bar(batch_labels, file_df[col], bottom=bottom, label=col.replace('_time', ''), color=color, alpha=0.8)\n",
        "            bottom += file_df[col]\n",
        "        \n",
        "        ax2.set_xlabel('Batch Size')\n",
        "        ax2.set_ylabel('Time (s)')\n",
        "        ax2.set_title(f'Time Breakdown by Step ({first_file[:30]})')\n",
        "        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. Read batches time vs number of batches\n",
        "    ax3 = axes[1, 0]\n",
        "    for filename in df_results['filename'].unique():\n",
        "        file_df = df_results[df_results['filename'] == filename].copy()\n",
        "        ax3.scatter(file_df['num_batches'], file_df['read_batches_time'], \n",
        "                   label=filename[:30], alpha=0.7, s=100)\n",
        "    \n",
        "    ax3.set_xlabel('Number of Batches')\n",
        "    ax3.set_ylabel('Read Batches Time (s)')\n",
        "    ax3.set_title('Read Time vs Number of Batches')\n",
        "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Concatenate time vs number of batches\n",
        "    ax4 = axes[1, 1]\n",
        "    for filename in df_results['filename'].unique():\n",
        "        file_df = df_results[df_results['filename'] == filename].copy()\n",
        "        ax4.scatter(file_df['num_batches'], file_df['concatenate_time'], \n",
        "                   label=filename[:30], alpha=0.7, s=100)\n",
        "    \n",
        "    ax4.set_xlabel('Number of Batches')\n",
        "    ax4.set_ylabel('Concatenate Time (s)')\n",
        "    ax4.set_title('Concatenate Time vs Number of Batches')\n",
        "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"KEY INSIGHTS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Find bottlenecks\n",
        "    for filename in df_results['filename'].unique():\n",
        "        file_df = df_results[df_results['filename'] == filename].copy()\n",
        "        avg_read = file_df['read_batches_time'].mean()\n",
        "        avg_concat = file_df['concatenate_time'].mean()\n",
        "        avg_zip = file_df['zip_objects_time'].mean()\n",
        "        avg_total = file_df['total_time'].mean()\n",
        "        \n",
        "        print(f\"\\nðŸ“ {filename}:\")\n",
        "        print(f\"   Read batches: {avg_read:.2f}s ({avg_read/avg_total*100:.1f}% of total)\")\n",
        "        print(f\"   Concatenate: {avg_concat:.2f}s ({avg_concat/avg_total*100:.1f}% of total)\")\n",
        "        print(f\"   Zip objects: {avg_zip:.2f}s ({avg_zip/avg_total*100:.1f}% of total)\")\n",
        "        \n",
        "        if avg_concat > avg_read * 0.5:\n",
        "            print(f\"   âš ï¸  WARNING: Concatenate time is significant! This might be a bottleneck.\")\n",
        "        if avg_zip > avg_read * 0.3:\n",
        "            print(f\"   âš ï¸  WARNING: Zip time is significant! This might be a bottleneck.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with Very Small Batch Sizes (Stress Test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STRESS TEST: Very Small Batch Sizes\n",
            "================================================================================\n",
            "False\n",
            "\n",
            "================================================================================\n",
            "Testing file: 2024r-pp_mc_b1c18ef224d66cb7.root\n",
            "File size: 0.01 MB\n",
            "Number of entries: 20,202,169\n",
            "================================================================================\n",
            "âŒ Batch size 1000: Error - AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3931480/1153906297.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3931480/1496193027.py\", line 34, in parse_root_file_instrumented\n",
            "    obj_branches_and_quantities = parser.AtlasOpenParser.extract_branches_by_obj_in_schema(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Batch size 5000: Error - AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3931480/1153906297.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3931480/1496193027.py\", line 34, in parse_root_file_instrumented\n",
            "    obj_branches_and_quantities = parser.AtlasOpenParser.extract_branches_by_obj_in_schema(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3931480/1153906297.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3931480/1496193027.py\", line 34, in parse_root_file_instrumented\n",
            "    obj_branches_and_quantities = parser.AtlasOpenParser.extract_branches_by_obj_in_schema(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3931480/1153906297.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3931480/1496193027.py\", line 34, in parse_root_file_instrumented\n",
            "    obj_branches_and_quantities = parser.AtlasOpenParser.extract_branches_by_obj_in_schema(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Batch size 10000: Error - AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n",
            "âŒ Batch size 20000: Error - AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n",
            "âŒ Batch size 40000: Error - AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_3931480/1153906297.py\", line 36, in test_batch_sizes\n",
            "    result, timings = parse_root_file_instrumented(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_3931480/1496193027.py\", line 34, in parse_root_file_instrumented\n",
            "    obj_branches_and_quantities = parser.AtlasOpenParser.extract_branches_by_obj_in_schema(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: AtlasOpenParser.extract_branches_by_obj_in_schema() got an unexpected keyword argument 'schema'\n"
          ]
        }
      ],
      "source": [
        "# Test with very small batch sizes to see if batching overhead becomes significant\n",
        "if test_files:\n",
        "    filename, _ = test_files[0]  # Use first test file\n",
        "    file_path = os.path.join(input_dir, filename)\n",
        "    \n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"STRESS TEST: Very Small Batch Sizes\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        small_batch_sizes = [1_000, 5_000, 10_000, 20_000, 40_000]\n",
        "        stress_results = test_batch_sizes(file_path, filename, batch_sizes=small_batch_sizes)\n",
        "        \n",
        "        if stress_results:\n",
        "            df_stress = pd.DataFrame(stress_results)\n",
        "            \n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"STRESS TEST RESULTS\")\n",
        "            print(\"=\"*80)\n",
        "            \n",
        "            # Show how overhead scales with number of batches\n",
        "            print(\"\\nBatch Size | Num Batches | Total Time | Read Time | Concat Time | Overhead %\")\n",
        "            print(\"-\" * 80)\n",
        "            \n",
        "            for _, row in df_stress.iterrows():\n",
        "                overhead = (row['concatenate_time'] + row['zip_objects_time']) / row['total_time'] * 100\n",
        "                print(f\"{row['batch_size']:>10} | {row['num_batches']:>11} | {row['total_time']:>10.2f}s | \"\n",
        "                      f\"{row['read_batches_time']:>9.2f}s | {row['concatenate_time']:>10.2f}s | {overhead:>9.1f}%\")\n",
        "            \n",
        "            # Visualize\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "            \n",
        "            # Time vs batch size\n",
        "            ax1.plot(df_stress['batch_size'], df_stress['total_time'], marker='o', linewidth=2, markersize=8)\n",
        "            ax1.set_xlabel('Batch Size')\n",
        "            ax1.set_ylabel('Total Time (s)')\n",
        "            ax1.set_title('Total Time vs Batch Size (Stress Test)')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Overhead vs number of batches\n",
        "            overhead_pct = (df_stress['concatenate_time'] + df_stress['zip_objects_time']) / df_stress['total_time'] * 100\n",
        "            ax2.scatter(df_stress['num_batches'], overhead_pct, s=100, alpha=0.7)\n",
        "            ax2.set_xlabel('Number of Batches')\n",
        "            ax2.set_ylabel('Overhead % (Concat + Zip)')\n",
        "            ax2.set_title('Batching Overhead vs Number of Batches')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "else:\n",
        "    print(\"No test files available.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
