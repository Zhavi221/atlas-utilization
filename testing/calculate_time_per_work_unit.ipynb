{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate TIME_FOR_WORK_UNIT_SEC\n",
        "\n",
        "This notebook calculates the optimal `TIME_FOR_WORK_UNIT_SEC` parameter based on real data measurements. This value is used in `submit_im_jobs.sh` to determine how many batch jobs are needed.\n",
        "\n",
        "A work unit = 1 file × 1 combination × 1 final state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import awkward as ak\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import random\n",
        "\n",
        "from src.parse_atlas import parser\n",
        "from src.calculations import combinatorics\n",
        "from src.im_calculator.im_calculator import IMCalculator\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  Input directory: /storage/agrp/netalev/data/root_files/\n",
            "  Objects to calculate: ['Electrons', 'Muons', 'Jets', 'Photons']\n",
            "  Min/Max particles: 2-4\n",
            "  Min/Max count: 2-4\n",
            "\n",
            "Total combinations: 28512\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "config_path = 'configs/pipeline_config.yaml'\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "im_config = config['mass_calculate']\n",
        "input_dir = im_config['input_dir']\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Input directory: {input_dir}\")\n",
        "print(f\"  Objects to calculate: {im_config['objects_to_calculate']}\")\n",
        "print(f\"  Min/Max particles: {im_config['min_particles']}-{im_config['max_particles']}\")\n",
        "print(f\"  Min/Max count: {im_config['min_count']}-{im_config['max_count']}\")\n",
        "\n",
        "# Get all combinations\n",
        "all_combinations = combinatorics.get_all_combinations(\n",
        "    im_config[\"objects_to_calculate\"],\n",
        "    min_particles=im_config[\"min_particles\"],\n",
        "    max_particles=im_config[\"max_particles\"],\n",
        "    min_count=im_config[\"min_count\"],\n",
        "    max_count=im_config[\"max_count\"],\n",
        "    limit=im_config.get(\"limit_combinations\")\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal combinations: {len(all_combinations)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Files for Measurement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14 ROOT files\n",
            "\n",
            "Sampling 10 files for measurement:\n",
            "  2024r-pp_0f75d28651db83f5.root: 388.5 MB\n",
            "  2024r-pp_1da876f9c54974c0.root: 515.7 MB\n",
            "  2024r-pp_0fced08cf06527b7.root: 86.1 MB\n",
            "  2024r-pp_bc43f0e26da96200.root: 5.0 MB\n",
            "  2024r-pp_mc_5742f42bd168c3c5.root: 5.7 MB\n",
            "  ... and 5 more\n"
          ]
        }
      ],
      "source": [
        "# Get ROOT files\n",
        "root_files = [f for f in os.listdir(input_dir) if f.endswith(\".root\")]\n",
        "print(f\"Found {len(root_files)} ROOT files\")\n",
        "\n",
        "# Sample files for measurement (stratified by size if possible)\n",
        "sample_size = min(10, len(root_files))  # Sample 10 files or all if less\n",
        "\n",
        "# Get file sizes and sample\n",
        "file_sizes = [(f, os.path.getsize(os.path.join(input_dir, f))) for f in root_files]\n",
        "file_sizes.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Sample from different size ranges\n",
        "if len(file_sizes) >= sample_size:\n",
        "    # Take some large, some medium, some small\n",
        "    large = file_sizes[:len(file_sizes)//3]\n",
        "    medium = file_sizes[len(file_sizes)//3:2*len(file_sizes)//3]\n",
        "    small = file_sizes[2*len(file_sizes)//3:]\n",
        "    \n",
        "    sample_files = (\n",
        "        random.sample(large, min(sample_size//3, len(large))) +\n",
        "        random.sample(medium, min(sample_size//3, len(medium))) +\n",
        "        random.sample(small, min(sample_size - 2*(sample_size//3), len(small)))\n",
        "    )\n",
        "else:\n",
        "    sample_files = file_sizes\n",
        "\n",
        "sample_filenames = [f[0] for f in sample_files]\n",
        "print(f\"\\nSampling {len(sample_filenames)} files for measurement:\")\n",
        "for filename, size_bytes in sample_files[:5]:\n",
        "    print(f\"  {filename}: {size_bytes/(1024**2):.1f} MB\")\n",
        "if len(sample_files) > 5:\n",
        "    print(f\"  ... and {len(sample_files)-5} more\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure Time per Work Unit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Measuring work unit times...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:   0%|                      | 0/10 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "def measure_work_unit_time(file_path, filename, all_combinations, config):\n",
        "    \"\"\"\n",
        "    Measure time for processing one work unit (file × combination × final_state).\n",
        "    Returns list of measurements.\n",
        "    \"\"\"\n",
        "    measurements = []\n",
        "    \n",
        "    try:\n",
        "        # Parse file\n",
        "        parse_start = time.time()\n",
        "        particle_arrays = parser.AtlasOpenParser.parse_root_file(file_path)\n",
        "        parse_time = time.time() - parse_start\n",
        "        \n",
        "        if particle_arrays is None or len(particle_arrays) == 0:\n",
        "            return measurements\n",
        "        \n",
        "        # Initialize calculator\n",
        "        calculator = IMCalculator(particle_arrays)\n",
        "        \n",
        "        # Process each final state\n",
        "        for final_state, fs_events in calculator.group_by_final_state():\n",
        "            if len(fs_events) == 0:\n",
        "                continue\n",
        "            \n",
        "            # For each combination that matches this final state\n",
        "            for combination in all_combinations:\n",
        "                if not calculator.does_final_state_contain_combination(final_state, combination):\n",
        "                    continue\n",
        "                \n",
        "                # Measure time for this work unit\n",
        "                work_start = time.time()\n",
        "                \n",
        "                # Filter\n",
        "                filtered = calculator.filter_by_particle_counts(\n",
        "                    fs_events, combination, is_exact_count=True\n",
        "                )\n",
        "                if len(filtered) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # Slice\n",
        "                field_to_slice_by = config.get(\"field_to_slice_by\", \"pt\")\n",
        "                sliced = calculator.slice_by_field(\n",
        "                    filtered, combination, field_to_slice_by\n",
        "                )\n",
        "                if len(sliced) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # Calculate invariant mass\n",
        "                inv_mass = calculator.calculate_invariant_mass(sliced)\n",
        "                if not ak.any(inv_mass):\n",
        "                    continue\n",
        "                \n",
        "                work_time = time.time() - work_start\n",
        "                \n",
        "                measurements.append({\n",
        "                    'filename': filename,\n",
        "                    'final_state': final_state,\n",
        "                    'combination': str(combination),\n",
        "                    'parse_time_sec': parse_time,\n",
        "                    'work_time_sec': work_time,\n",
        "                    'num_events': len(fs_events),\n",
        "                    'num_mass_values': len(inv_mass)\n",
        "                })\n",
        "        \n",
        "        # Clean up\n",
        "        del particle_arrays, calculator\n",
        "        gc.collect()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "    \n",
        "    return measurements\n",
        "\n",
        "# Measure work unit times\n",
        "print(\"Measuring work unit times...\")\n",
        "all_measurements = []\n",
        "\n",
        "for filename in tqdm(sample_filenames, desc=\"Processing files\"):\n",
        "    file_path = os.path.join(input_dir, filename)\n",
        "    measurements = measure_work_unit_time(file_path, filename, all_combinations, im_config)\n",
        "    all_measurements.extend(measurements)\n",
        "    \n",
        "    if measurements:\n",
        "        avg_time = np.mean([m['work_time_sec'] for m in measurements])\n",
        "        print(f\"  {filename}: {len(measurements)} work units, avg {avg_time:.4f}s per unit\")\n",
        "\n",
        "if not all_measurements:\n",
        "    print(\"⚠️  No measurements collected!\")\n",
        "else:\n",
        "    df = pd.DataFrame(all_measurements)\n",
        "    print(f\"\\n✓ Collected {len(df)} work unit measurements\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Statistics and Recommended TIME_FOR_WORK_UNIT_SEC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(all_measurements) > 0:\n",
        "    df = pd.DataFrame(all_measurements)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    work_times = df['work_time_sec'].values\n",
        "    \n",
        "    stats = {\n",
        "        'mean': np.mean(work_times),\n",
        "        'median': np.median(work_times),\n",
        "        'std': np.std(work_times),\n",
        "        'min': np.min(work_times),\n",
        "        'max': np.max(work_times),\n",
        "        'p25': np.percentile(work_times, 25),\n",
        "        'p75': np.percentile(work_times, 75),\n",
        "        'p95': np.percentile(work_times, 95),\n",
        "        'p99': np.percentile(work_times, 99)\n",
        "    }\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"WORK UNIT TIME STATISTICS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Number of measurements: {len(work_times)}\")\n",
        "    print(f\"\\nTime per work unit (seconds):\")\n",
        "    print(f\"  Mean:   {stats['mean']:.4f}\")\n",
        "    print(f\"  Median: {stats['median']:.4f}\")\n",
        "    print(f\"  Std:    {stats['std']:.4f}\")\n",
        "    print(f\"  Min:    {stats['min']:.4f}\")\n",
        "    print(f\"  Max:    {stats['max']:.4f}\")\n",
        "    print(f\"\\nPercentiles:\")\n",
        "    print(f\"  25th:   {stats['p25']:.4f}\")\n",
        "    print(f\"  75th:   {stats['p75']:.4f}\")\n",
        "    print(f\"  95th:   {stats['p95']:.4f}\")\n",
        "    print(f\"  99th:   {stats['p99']:.4f}\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RECOMMENDATIONS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Conservative estimate (p95 to handle outliers)\n",
        "    recommended_p95 = stats['p95']\n",
        "    recommended_mean = stats['mean']\n",
        "    recommended_median = stats['median']\n",
        "    \n",
        "    print(f\"\\n1. Conservative estimate (95th percentile): {recommended_p95:.4f} seconds\")\n",
        "    print(f\"   → Use this for safety margin: handles 95% of cases\")\n",
        "    \n",
        "    print(f\"\\n2. Mean estimate: {recommended_mean:.4f} seconds\")\n",
        "    print(f\"   → Use this for average case estimation\")\n",
        "    \n",
        "    print(f\"\\n3. Median estimate: {recommended_median:.4f} seconds\")\n",
        "    print(f\"   → Use this if data has outliers\")\n",
        "    \n",
        "    # Account for parsing time\n",
        "    if 'parse_time_sec' in df.columns:\n",
        "        avg_parse_time = df.groupby('filename')['parse_time_sec'].first().mean()\n",
        "        print(f\"\\n4. Average parse time per file: {avg_parse_time:.2f} seconds\")\n",
        "        print(f\"   → This is one-time cost per file, not per work unit\")\n",
        "        \n",
        "        # Calculate total time including parsing overhead\n",
        "        # Parse time is amortized across all work units in a file\n",
        "        avg_work_units_per_file = df.groupby('filename').size().mean()\n",
        "        parse_overhead_per_unit = avg_parse_time / avg_work_units_per_file if avg_work_units_per_file > 0 else 0\n",
        "        \n",
        "        print(f\"\\n5. Parse overhead per work unit: {parse_overhead_per_unit:.4f} seconds\")\n",
        "        print(f\"   → Average work units per file: {avg_work_units_per_file:.1f}\")\n",
        "        \n",
        "        total_recommended = recommended_p95 + parse_overhead_per_unit\n",
        "        print(f\"\\n6. Total recommended (work + parse overhead): {total_recommended:.4f} seconds\")\n",
        "    \n",
        "    # Final recommendation\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RECOMMENDATION\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Use p95 for safety, but add some buffer\n",
        "    final_recommendation = max(recommended_p95 * 1.2, recommended_mean * 1.5)\n",
        "    \n",
        "    print(f\"\\nRecommended TIME_FOR_WORK_UNIT_SEC: {final_recommendation:.4f} seconds\")\n",
        "    print(f\"\\nThis value accounts for:\")\n",
        "    print(f\"  - 95th percentile work unit time: {recommended_p95:.4f}s\")\n",
        "    print(f\"  - Safety margin (20-50%): {final_recommendation - recommended_p95:.4f}s\")\n",
        "    if 'parse_time_sec' in df.columns:\n",
        "        print(f\"  - Parse overhead: {parse_overhead_per_unit:.4f}s per work unit\")\n",
        "    \n",
        "    print(f\"\\nTo use in submit_im_jobs.sh:\")\n",
        "    print(f\"  --time_for_work_unit_sec {final_recommendation:.4f}\")\n",
        "    \n",
        "    # Save to file\n",
        "    output_file = '../execute_as_batch_jobs/recommended_time_per_work_unit.txt'\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(f\"{final_recommendation:.4f}\\n\")\n",
        "    print(f\"\\n✓ Saved recommendation to: {output_file}\")\n",
        "else:\n",
        "    print(\"⚠️  No measurements to analyze!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(all_measurements) > 0:\n",
        "    df = pd.DataFrame(all_measurements)\n",
        "    work_times = df['work_time_sec'].values\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Histogram\n",
        "    axes[0].hist(work_times, bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(np.mean(work_times), color='red', linestyle='--', label=f'Mean: {np.mean(work_times):.4f}s')\n",
        "    axes[0].axvline(np.median(work_times), color='green', linestyle='--', label=f'Median: {np.median(work_times):.4f}s')\n",
        "    axes[0].axvline(np.percentile(work_times, 95), color='orange', linestyle='--', label=f'95th: {np.percentile(work_times, 95):.4f}s')\n",
        "    axes[0].set_xlabel('Time per Work Unit (seconds)')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('Distribution of Work Unit Times')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Log scale if needed\n",
        "    if np.max(work_times) / np.min(work_times[work_times > 0]) > 100:\n",
        "        axes[1].hist(work_times, bins=50, edgecolor='black', alpha=0.7)\n",
        "        axes[1].set_xscale('log')\n",
        "        axes[1].set_xlabel('Time per Work Unit (seconds, log scale)')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].set_title('Distribution (Log Scale)')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        # Box plot\n",
        "        axes[1].boxplot(work_times, vert=True)\n",
        "        axes[1].set_ylabel('Time per Work Unit (seconds)')\n",
        "        axes[1].set_title('Box Plot of Work Unit Times')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Show breakdown by file\n",
        "    if 'filename' in df.columns:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"BREAKDOWN BY FILE\")\n",
        "        print(\"=\"*80)\n",
        "        file_stats = df.groupby('filename')['work_time_sec'].agg(['mean', 'std', 'count'])\n",
        "        file_stats.columns = ['Avg Time (s)', 'Std Dev (s)', 'Work Units']\n",
        "        print(file_stats.to_string())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
