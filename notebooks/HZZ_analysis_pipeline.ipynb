{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23ee171",
   "metadata": {},
   "source": [
    "# ATLAS Open Data $H\\rightarrow ZZ^\\star$ with `ServiceX`, `coffea`, `cabinetry` & `pyhf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b3fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "import hist\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import pyhf\n",
    "import uproot\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.schemas.base import BaseSchema\n",
    "import utils\n",
    "from utils import infofile  # contains cross-section information\n",
    "\n",
    "import servicex\n",
    "\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "\n",
    "utils.clean_up()  # delete output from previous runs of notebook (optional)\n",
    "utils.set_logging()  # configure logging output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b53ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some global settings\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# scaling for local setups with FuturesExecutor\n",
    "NUM_CORES = 4\n",
    "\n",
    "# ServiceX behavior: ignore cache with repeated queries\n",
    "IGNORE_CACHE = False\n",
    "\n",
    "# ServiceX behavior: choose query language\n",
    "USE_SERVICEX_UPROOT_RAW = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551acd1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We are going to use the ATLAS Open Data for this demonstration, in particular a $H\\rightarrow ZZ^\\star$ analysis. Find more information on the [ATLAS Open Data documentation](http://opendata.atlas.cern/release/2020/documentation/physics/FL2.html) and in [ATL-OREACH-PUB-2020-001](https://cds.cern.ch/record/2707171). The datasets used are [10.7483/OPENDATA.ATLAS.2Y1T.TLGL](http://doi.org/10.7483/OPENDATA.ATLAS.2Y1T.TLGL). The material in this notebook is based on the [ATLAS Open Data notebooks](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata), a [PyHEP 2021 ServiceX demo](https://github.com/gordonwatts/pyhep-2021-SX-OpenDataDemo), and [Storm Lin's adoption](https://github.com/stormsomething/CoffeaHZZAnalysis) of this analysis.\n",
    "\n",
    "This notebook is meant as a **technical demonstration**. In particular, the systematic uncertainties defined are purely to demonstrate technical aspects of realistic workflows, and are not meant to be meaningful physically. The fit performed to data consequently also only demonstrate technical aspects. If you are interested about the physics of $H\\rightarrow ZZ^\\star$, check out for example the actual ATLAS cross-section measurement: [Eur. Phys. J. C 80 (2020) 942](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/HIGG-2018-29/).\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](utils/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790c63f",
   "metadata": {},
   "source": [
    "### Tools and packages used in this example\n",
    "\n",
    "The notebook showcases:\n",
    "- data delivery with `ServiceX`\n",
    "- event / column selection with `func_adl`\n",
    "- data handling with `awkward-array`\n",
    "- histogram production with `coffea`\n",
    "- histogram handling with `hist`\n",
    "- visualization with `mplhep`, `hist` & `matplotlib`\n",
    "- ROOT file handling with `uproot`\n",
    "- statistical model construction with `cabinetry`\n",
    "- statistical inference with `pyhf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f9d84",
   "metadata": {},
   "source": [
    "### High-level strategy\n",
    "\n",
    "We will define which files to process, set up a query with `func_adl` to extract data provided by `ServiceX`, and use `coffea` to construct histograms.\n",
    "Those histograms will be saved with `uproot`, and then assembled into a statistical model with `cabinetry`.\n",
    "Following that, we perform statistical inference with `pyhf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c9ef0",
   "metadata": {},
   "source": [
    "### Required setup for this notebook\n",
    "\n",
    "If you are running on the coffea-casa Open Data instance, ServiceX credentials are automatically available to you.\n",
    "Otherwise you will need to set those up.\n",
    "Create a file `servicex.yaml` in your home directory, or the place this notebook is located in.\n",
    "\n",
    "See this [talk by KyungEon](https://indico.cern.ch/event/1076231/contributions/4560404/) and the [ServiceX doc](https://servicex.readthedocs.io/en/latest/user/getting-started/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6c272",
   "metadata": {},
   "source": [
    "## Files to process\n",
    "\n",
    "To get started, we define which files are going to be processed in this notebook.\n",
    "We also set some information for histogramming that will be used subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24197a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = (\n",
    "    \"http://xrootd-local.unl.edu:1094//store/user/AGC/ATLAS_HZZ/\"\n",
    ")\n",
    "\n",
    "# labels for combinations of datasets\n",
    "z_ttbar = r\"Background $Z,t\\bar{t}$\"\n",
    "zzstar = r\"Background $ZZ^{\\star}$\"\n",
    "signal = r\"Signal ($m_H$ = 125 GeV)\"\n",
    "\n",
    "input_files = {\n",
    "    \"Data\": [\n",
    "        prefix + \"data_A.4lep.root\",\n",
    "        prefix + \"data_B.4lep.root\",\n",
    "        prefix + \"data_C.4lep.root\",\n",
    "        prefix + \"data_D.4lep.root\",\n",
    "    ],\n",
    "    z_ttbar: [\n",
    "        prefix + \"mc_361106.Zee.4lep.root\",\n",
    "        prefix + \"mc_361107.Zmumu.4lep.root\",\n",
    "        prefix + \"mc_410000.ttbar_lep.4lep.root\",\n",
    "    ],\n",
    "    zzstar: [prefix + \"mc_363490.llll.4lep.root\"],\n",
    "    signal: [\n",
    "        prefix + \"mc_345060.ggH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"mc_344235.VBFH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"mc_341964.WH125_ZZ4lep.4lep.root\",\n",
    "        prefix + \"mc_341947.ZH125_ZZ4lep.4lep.root\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# information for histograms\n",
    "bin_edge_low = 80  # 80 GeV\n",
    "bin_edge_high = 250  # 250 GeV\n",
    "num_bins = 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bcc6e3",
   "metadata": {},
   "source": [
    "## Setting up a query with `func_adl`\n",
    "\n",
    "We are using `func_adl` for event & column selection, and make a datasource with the query built by `get_lepton_query`.\n",
    "\n",
    "A list of all available columns in the input files can be found in the [ATLAS documentation of branches](http://opendata.atlas.cern/release/2020/documentation/datasets/dataset13.html).\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> scale factor variation, applied already at event selection stage. Imagine that this could be a calculation that requires a lot of different variables which are no longer needed downstream afterwards, so it makes sense to do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31361726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lepton_query():\n",
    "    \"\"\"Performs event selection with func_adl transformer: require events with exactly four leptons.\n",
    "    Also select all columns needed further downstream for processing &\n",
    "    histogram filling.\n",
    "    \"\"\"\n",
    "    from servicex import query as q\n",
    "    return q.FuncADL_Uproot().FromTree('mini')\\\n",
    "        .Where(lambda event: event.lep_n == 4).Select(\n",
    "        lambda e: {\n",
    "            \"lep_pt\": e.lep_pt,\n",
    "            \"lep_eta\": e.lep_eta,\n",
    "            \"lep_phi\": e.lep_phi,\n",
    "            \"lep_energy\": e.lep_E,\n",
    "            \"lep_charge\": e.lep_charge,\n",
    "            \"lep_typeid\": e.lep_type,\n",
    "            \"mcWeight\": e.mcWeight,\n",
    "            \"scaleFactor\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP,\n",
    "            # scale factor systematic variation example\n",
    "            \"scaleFactorUP\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP\n",
    "            * 1.1,\n",
    "            \"scaleFactorDOWN\": e.scaleFactor_ELE\n",
    "            * e.scaleFactor_MUON\n",
    "            * e.scaleFactor_LepTRIGGER\n",
    "            * e.scaleFactor_PILEUP\n",
    "            * 0.9,\n",
    "        }\n",
    "    )\n",
    "\n",
    "def get_lepton_query_uproot_raw():\n",
    "    \"\"\"Performs event selection with uproot-raw transformer: require events with exactly four leptons.\n",
    "    Also select all columns needed further downstream for processing &\n",
    "    histogram filling.\n",
    "    \"\"\"\n",
    "    from servicex import query as q\n",
    "    return q.UprootRaw([{'treename': 'mini',\n",
    "                         'expressions': ['lep_pt', 'lep_eta', 'lep_phi', 'lep_energy', 'lep_charge', \n",
    "                                         'lep_typeid', 'mcWeight', 'scaleFactor', 'scaleFactorUP', 'scaleFactorDOWN'],\n",
    "                         'aliases': { 'lep_typeid': 'lep_type', 'lep_energy': 'lep_E',\n",
    "                                      'scaleFactor': 'scaleFactor_ELE*scaleFactor_MUON*scaleFactor_LepTRIGGER*scaleFactor_PILEUP',\n",
    "                                      'scaleFactorUP': 'scaleFactor*1.1',\n",
    "                                      'scaleFactorDOWN': 'scaleFactor*0.9' }\n",
    "                        }])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddb0f0",
   "metadata": {},
   "source": [
    "# Caching the queried datasets with `ServiceX`\n",
    "\n",
    "Using the queries created with `func_adl`, we are using `ServiceX` to read the ATLAS Open Data files to build cached files with only the specific event information as dictated by the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d3663",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# create the query\n",
    "if USE_SERVICEX_UPROOT_RAW:\n",
    "    query = get_lepton_query_uproot_raw()\n",
    "else:\n",
    "    query = get_lepton_query()\n",
    "\n",
    "# now we query the files and create a fileset dictionary containing the\n",
    "# URLs pointing to the queried files\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "fileset = {}\n",
    "\n",
    "bundle = { 'General': { 'Delivery': 'URLs' },\n",
    "           'Sample': [ { 'Name': ds_name,\n",
    "                          'Query': query,\n",
    "                          'Dataset': servicex.dataset.FileList(input_files[ds_name]),\n",
    "                          'IgnoreLocalCache': IGNORE_CACHE } for ds_name in input_files.keys() ]\n",
    "           }\n",
    "\n",
    "results = servicex.deliver(bundle)\n",
    "fileset = { _: {\"files\": results[_], \"metadata\": {\"dataset_name\": _}} for _ in results }\n",
    "\n",
    "print(f\"execution took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d9c5f",
   "metadata": {},
   "source": [
    "We now have a fileset dictionary containing the addresses of the queried files, ready to pass to `coffea`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf95576",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53de594",
   "metadata": {},
   "source": [
    "## Processing `ServiceX`-provided data with `coffea`\n",
    "\n",
    "Event weighting: look up cross-section from a provided utility file, and correctly normalize all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2a6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample: str) -> float:\n",
    "    \"\"\"Returns normalization weight for a given sample.\"\"\"\n",
    "    lumi = 10_000  # pb^-1\n",
    "    xsec_map = infofile.infos[sample]  # dictionary with event weighting information\n",
    "    xsec_weight = (lumi * xsec_map[\"xsec\"]) / (xsec_map[\"sumw\"] * xsec_map[\"red_eff\"])\n",
    "    return xsec_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b14ccb",
   "metadata": {},
   "source": [
    "Cuts to apply:\n",
    "- two opposite flavor lepton pairs (total lepton charge is 0)\n",
    "- lepton types: 4 electrons, 4 muons, or 2 electrons + 2 muons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1515ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lepton_filter(lep_charge, lep_type):\n",
    "    \"\"\"Filters leptons: sum of charges is required to be 0, and sum of lepton types 44/48/52.\n",
    "    Electrons have type 11, muons have 13, so this means 4e/4mu/2e2mu.\n",
    "    \"\"\"\n",
    "    sum_lep_charge = ak.sum(lep_charge, axis=1)\n",
    "    sum_lep_type = ak.sum(lep_type, axis=1)\n",
    "    good_lep_type = ak.any(\n",
    "        [sum_lep_type == 44, sum_lep_type == 48, sum_lep_type == 52], axis=0\n",
    "    )\n",
    "    return ak.all([sum_lep_charge == 0, good_lep_type], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfd064",
   "metadata": {},
   "source": [
    "Set up the `coffea` processor. It will apply cuts, calculate the four-lepton invariant mass, and fill a histogram.\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> m4l variation, applied in the processor to remaining events. This might instead for example be the result of applying a tool performing a computationally expensive calculation, which should only be run for events where it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f451f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HZZAnalysis(processor.ProcessorABC):\n",
    "    \"\"\"The coffea processor used in this analysis.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, events):\n",
    "        vector.register_awkward()\n",
    "        # type of dataset being processed, provided via metadata (comes originally from fileset)\n",
    "        dataset_category = events.metadata[\"dataset_name\"]\n",
    "\n",
    "        # apply a cut to events, based on lepton charge and lepton type\n",
    "        events = events[lepton_filter(events.lep_charge, events.lep_typeid)]\n",
    "\n",
    "        # construct lepton four-vectors\n",
    "        leptons = ak.zip(\n",
    "            {\"pt\": events.lep_pt,\n",
    "             \"eta\": events.lep_eta,\n",
    "             \"phi\": events.lep_phi,\n",
    "             \"energy\": events.lep_energy},\n",
    "            with_name=\"Momentum4D\",\n",
    "        )\n",
    "\n",
    "        # calculate the 4-lepton invariant mass for each remaining event\n",
    "        # this could also be an expensive calculation using external tools\n",
    "        mllll = (\n",
    "            leptons[:, 0] + leptons[:, 1] + leptons[:, 2] + leptons[:, 3]\n",
    "        ).mass / 1000\n",
    "\n",
    "        # creat histogram holding outputs, for data just binned in m4l\n",
    "        mllllhist_data = hist.Hist.new.Reg(\n",
    "            num_bins,\n",
    "            bin_edge_low,\n",
    "            bin_edge_high,\n",
    "            name=\"mllll\",\n",
    "            label=\"$\\mathrm{m_{4l}}$ [GeV]\",\n",
    "        ).Weight()  # using weighted storage here for plotting later, but not needed\n",
    "\n",
    "        # three histogram axes for MC: m4l, category, and variation (nominal and\n",
    "        # systematic variations)\n",
    "        mllllhist_MC = (\n",
    "            hist.Hist.new.Reg(\n",
    "                num_bins,\n",
    "                bin_edge_low,\n",
    "                bin_edge_high,\n",
    "                name=\"mllll\",\n",
    "                label=\"$\\mathrm{m_{4l}}$ [GeV]\",\n",
    "            )\n",
    "            .StrCat([k for k in fileset.keys() if k != \"Data\"], name=\"dataset\")\n",
    "            .StrCat(\n",
    "                [\"nominal\", \"scaleFactorUP\", \"scaleFactorDOWN\", \"m4lUP\", \"m4lDOWN\"],\n",
    "                name=\"variation\",\n",
    "            )\n",
    "            .Weight()\n",
    "        )\n",
    "\n",
    "        if dataset_category == \"Data\":\n",
    "            # create and fill a histogram for m4l\n",
    "            mllllhist_data.fill(mllll=mllll)\n",
    "\n",
    "        else:\n",
    "            # extract the sample name from the filename to calculate x-sec weight\n",
    "            sample = re.findall(r\"mc_\\d+\\.(.+)\\.4lep\", events.metadata[\"filename\"])[0]\n",
    "            basic_weight = get_xsec_weight(sample) * events.mcWeight\n",
    "            totalWeights = basic_weight * events.scaleFactor\n",
    "\n",
    "            # calculate systematic variations for weight\n",
    "            totalWeightsUp = basic_weight * events.scaleFactorUP\n",
    "            totalWeightsDown = basic_weight * events.scaleFactorDOWN\n",
    "\n",
    "            # create and fill weighted histograms for m4l: nominal and variations\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"nominal\",\n",
    "                weight=totalWeights,\n",
    "            )\n",
    "\n",
    "            # scale factor variations\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"scaleFactorUP\",\n",
    "                weight=totalWeightsUp,\n",
    "            )\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"scaleFactorDOWN\",\n",
    "                weight=totalWeightsDown,\n",
    "            )\n",
    "\n",
    "            # variation in 4-lepton invariant mass\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll * 1.01,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"m4lUP\",\n",
    "                weight=totalWeights,\n",
    "            )\n",
    "            mllllhist_MC.fill(\n",
    "                mllll=mllll * 0.99,\n",
    "                dataset=dataset_category,\n",
    "                variation=\"m4lDOWN\",\n",
    "                weight=totalWeights,\n",
    "            )\n",
    "\n",
    "        return {\"data\": mllllhist_data, \"MC\": mllllhist_MC}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c2ad5",
   "metadata": {},
   "source": [
    "## Producing the desired histograms\n",
    "\n",
    "Run the processor on data previously gathered by ServiceX, then gather output histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "executor = processor.FuturesExecutor(workers=NUM_CORES)\n",
    "run = processor.Runner(executor=executor, savemetrics=True, metadata_cache={},\n",
    "                       chunksize=CHUNKSIZE, schema=BaseSchema)\n",
    "# The trees returned by ServiceX will have different names depending on the query language used\n",
    "all_histograms, metrics = run(fileset, \"mini\" if USE_SERVICEX_UPROOT_RAW else \"servicex\", processor_instance=HZZAnalysis())\n",
    "\n",
    "print(f\"execution took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62b89d",
   "metadata": {},
   "source": [
    "## Plotting histograms with `mplhep`, `hist` & `matplotlib`\n",
    "\n",
    "We can plot some of the histograms we just produced with `mplhep`, `hist` & `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms with mplhep & hist\n",
    "mplhep.histplot(\n",
    "    all_histograms[\"data\"], histtype=\"errorbar\", color=\"black\", label=\"Data\"\n",
    ")\n",
    "hist.Hist.plot1d(\n",
    "    all_histograms[\"MC\"][:, :, \"nominal\"],\n",
    "    stack=True,\n",
    "    histtype=\"fill\",\n",
    "    color=[\"purple\", \"red\", \"lightblue\"],\n",
    ")\n",
    "\n",
    "# plot band for MC statistical uncertainties via utility function\n",
    "# (this uses matplotlib directly)\n",
    "utils.plot_errorband(bin_edge_low, bin_edge_high, num_bins, all_histograms)\n",
    "\n",
    "# we are using a small utility function to also save the figure in .png and .pdf\n",
    "# format, you can find the produced figure in the figures/ folder\n",
    "utils.save_figure(\"m4l_stat_uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3d5b5",
   "metadata": {},
   "source": [
    "## Saving histograms with `uproot`\n",
    "\n",
    "In order to build a statistical model, we will use `cabinetry`'s support for reading histograms to build a so-called workspace specifying the model.\n",
    "We will save the histograms we just created to disk with `uproot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75837f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"histograms.root\"\n",
    "with uproot.recreate(file_name) as f:\n",
    "    f[\"data\"] = all_histograms[\"data\"]\n",
    "\n",
    "    f[\"Z_tt\"] = all_histograms[\"MC\"][:, z_ttbar, \"nominal\"]\n",
    "    f[\"Z_tt_SF_up\"] = all_histograms[\"MC\"][:, z_ttbar, \"scaleFactorUP\"]\n",
    "    f[\"Z_tt_SF_down\"] = all_histograms[\"MC\"][:, z_ttbar, \"scaleFactorDOWN\"]\n",
    "    f[\"Z_tt_m4l_up\"] = all_histograms[\"MC\"][:, z_ttbar, \"m4lUP\"]\n",
    "    f[\"Z_tt_m4l_down\"] = all_histograms[\"MC\"][:, z_ttbar, \"m4lDOWN\"]\n",
    "\n",
    "    f[\"ZZ\"] = all_histograms[\"MC\"][:, zzstar, \"nominal\"]\n",
    "    f[\"ZZ_SF_up\"] = all_histograms[\"MC\"][:, zzstar, \"scaleFactorUP\"]\n",
    "    f[\"ZZ_SF_down\"] = all_histograms[\"MC\"][:, zzstar, \"scaleFactorDOWN\"]\n",
    "    f[\"ZZ_m4l_up\"] = all_histograms[\"MC\"][:, zzstar, \"m4lUP\"]\n",
    "    f[\"ZZ_m4l_down\"] = all_histograms[\"MC\"][:, zzstar, \"m4lDOWN\"]\n",
    "\n",
    "    f[\"signal\"] = all_histograms[\"MC\"][:, signal, \"nominal\"]\n",
    "    f[\"signal_SF_up\"] = all_histograms[\"MC\"][:, signal, \"scaleFactorUP\"]\n",
    "    f[\"signal_SF_down\"] = all_histograms[\"MC\"][:, signal, \"scaleFactorDOWN\"]\n",
    "    f[\"signal_m4l_up\"] = all_histograms[\"MC\"][:, signal, \"m4lUP\"]\n",
    "    f[\"signal_m4l_down\"] = all_histograms[\"MC\"][:, signal, \"m4lDOWN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac08fe",
   "metadata": {},
   "source": [
    "## Building a workspace and running a fit with `cabinetry` & `pyhf`\n",
    "\n",
    "Take a look at the `cabinetry` configuration file in `config.yml`.\n",
    "It specifies the model to be built.\n",
    "In particular, it lists the samples we are going to be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"config.yml\")\n",
    "config[\"Samples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb046e9a",
   "metadata": {},
   "source": [
    "It also shows which systematic uncertainties we will apply.\n",
    "This includes a new systematic uncertainty defined at this stage.\n",
    "\n",
    "<span style=\"color:darkgreen\">**Systematic uncertainty added:**</span> $ZZ^\\star$ normalization; this does not require any histograms, so we can define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"Systematics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d1c08",
   "metadata": {},
   "source": [
    "The information in the configuration is used to construct a statistical model, the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f203255",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc73ee",
   "metadata": {},
   "source": [
    "Create a `pyhf` model and extract the data from the workspace. Perform a MLE fit, the results will be reported in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415afd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a391874",
   "metadata": {},
   "source": [
    "We can visualize the pulls and correlations.\n",
    "`cabinetry` saves this figure by default as a `.pdf`, but here we will use our small utility again to save in both `.png` and `.pdf` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"Signal_norm\", close_figure=False, save_figure=False\n",
    ")\n",
    "utils.save_figure(\"pulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28435ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry.visualize.correlation_matrix(\n",
    "    fit_results, pruning_threshold=0.15, close_figure=False, save_figure=False\n",
    ")\n",
    "utils.save_figure(\"correlation_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0738453",
   "metadata": {},
   "source": [
    "Finally, visualize the post-fit model and data.\n",
    "We first create the post-fit model prediction, using the model and the best-fit resuts.\n",
    "\n",
    "The visualization is using information stored in the workspace, which does not include binning or which observable is used.\n",
    "This information can be passed in via the `config` kwarg, but we can also edit the figure after its creation.\n",
    "We will demonstrate both approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create post-fit model prediction\n",
    "postfit_model = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "\n",
    "# binning to use in plot\n",
    "plot_config = {\n",
    "    \"Regions\": [\n",
    "        {\n",
    "            \"Name\": \"Signal_region\",\n",
    "            \"Binning\": list(np.linspace(bin_edge_low, bin_edge_high, num_bins + 1)),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "figure_dict = cabinetry.visualize.data_mc(\n",
    "    postfit_model, data, config=plot_config, save_figure=False\n",
    ")\n",
    "\n",
    "# modify x-axis label\n",
    "fig = figure_dict[0][\"figure\"]\n",
    "fig.axes[1].set_xlabel(\"m4l [GeV]\")\n",
    "\n",
    "# let's also save the figure\n",
    "utils.save_figure(\"Signal_region_postfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b96170",
   "metadata": {},
   "source": [
    "We can also use `pyhf` directly. We already have a model and data, so let's calculate the CLs value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_test = 1.0\n",
    "cls = float(pyhf.infer.hypotest(mu_test, data, model))\n",
    "print(f\"CL_S for Signal_norm={mu_test} is {cls:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58aaca4",
   "metadata": {},
   "source": [
    "## Final remarks: control flow and duplication of information\n",
    "\n",
    "In the demonstration today we first built histograms, making sure we produce all that we will need for our statistical model.\n",
    "We then built the model, saying where to find each histogram.\n",
    "If we want to add a new histogram-based systematic uncertainty, we will need to implement it in two places:\n",
    "- in the histogram-producing code (e.g. coffea processor),\n",
    "- in the model building instructions.\n",
    "\n",
    "It can be convenient to avoid this duplication, which can be achieved as follows:\n",
    "- specify all relevant information in the model building configuration,\n",
    "- use that information to steer the histogram production.\n",
    "\n",
    "This method is available in `cabinetry`, check out the [cabinetry tutorials repository](https://github.com/cabinetry/cabinetry-tutorials) to see it in action.\n",
    "With this approach `cabinetry` builds the histograms it needs itself.\n",
    "\n",
    "This works well for some cases, but not for others:\n",
    "- a simple cut can be specified easily in the configuration, and varied for systematic uncertainties,\n",
    "- a complex calculation cannot easily be captured in such a way.\n",
    "\n",
    "The image below describes different options.\n",
    "The general considerations here are independent of the exact tools used, and should apply equally when using similar workflows.\n",
    "\n",
    "![schematic for control flow options](utils/control_flow.png)\n",
    "\n",
    "A flexible design that works well for all scenarios is needed here, but is not available yet.\n",
    "If you have thoughts about this, please do get in touch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166163d",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are a few ideas to try out to become more familiar with the tools shown in this notebook:\n",
    "\n",
    "- Run the notebook a second time. It should be faster now â€” you are taking advantage of caching!\n",
    "- Change the event pre-selection in the `func_adl` query. Try out requiring exactly zero jets. Has the analysis become more or less sensitive after this change?\n",
    "- Change the lepton requirements. What happens when only accepting events with 4 electrons or 4 muons?\n",
    "- Try a different binning. Note how you still benefit from caching in the `ServiceX`-delivered data!\n",
    "- Compare the pre- and post-fit data/MC agreement (hint: the `fit_results` kwarg in `cabinetry.model_utils.prediction` is optional).\n",
    "- Find the value of the `Signal_norm` normalization factor for which CL_S = 0.05, the 95% confidence level upper parameter limit (hint: use `pyhf` directly, or `cabinetry.fit.limit`).\n",
    "- Separate the $Z$ and $t\\bar{t}$ backgrounds and add a 6% normalization uncertainty for $t\\bar{t}$ in the fit.\n",
    "- Replace the 10% normalization uncertainty for the $ZZ^\\star$ background by a free-floating normalization factor in the fit.\n",
    "\n",
    "\n",
    "Advanced ideas:\n",
    "- Implement a more realistic systematic uncertainty in the `coffea` processor, for example for detector-related uncertainties for lepton kinematics. Propagate it through the analysis chain to observe the impact in a ranking plot produced with `cabinetry`.\n",
    "- Try out this workflow with your own analysis! Are there missing features or ways to streamline the experience? If so, please let us know!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3893bc5b6e3c583e8433aeb5f586a02793edec63a5fa22ceec6853b31493c624"
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
